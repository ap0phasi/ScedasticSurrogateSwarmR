swarm_state <- step_swarm(desired_values = ydat,swarm_state,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=dim(swarm_state$x.p)[1],num_cluster=1,swarm.control = tempcontrol))
matplot(xdat,swarm_state$pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
surrogate_out <- surrogate_model(as.vector(swarm_state$poly_rec),swarm_state$polyouts,swarm_state$centersaves,deg=1)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(swarm_state$poly_rec)),col="red")
title("Scedastic Surrogate Swarm Optimization")
legend(par('usr')[2], par('usr')[4], bty='n',
xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(swarm_state$allxp)[1]))
model_function <-function(params,ls){
new_weights <- assign_keras_weights(params,weight_mat_orig)
newmodel <- model
set_weights(newmodel,new_weights)
modval <- newmodel %>% predict(xdat,verbose = F)
return(modval)
}
#Equality Constraints
eq <- function(params){
constraints = c(
)
return(constraints)
}
#Inequality Constraints
# of form <= 0
ineq <- function(params){
constraints = c(
)
return(constraints)
}
swarm_state <- initialize_swarm(desired_values = ydat,param_len = weight_num,lowlim=lowlim,highlim=highlim,ineq_w = c(),eq_w = c(),config = swarm.config(swarm_size = 100))
for (itt in 1:50){
tempcontrol = swarm.control(poly_w=0.2,stoch_w = 0.01)
swarm_state <- step_swarm(desired_values = ydat,swarm_state,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=dim(swarm_state$x.p)[1],num_cluster=1,swarm.control = tempcontrol))
matplot(xdat,swarm_state$pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
surrogate_out <- surrogate_model(as.vector(swarm_state$poly_rec),swarm_state$polyouts,swarm_state$centersaves,deg=1)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(swarm_state$poly_rec)),col="red")
title("Scedastic Surrogate Swarm Optimization")
legend(par('usr')[2], par('usr')[4], bty='n',
xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(swarm_state$allxp)[1]))
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 1,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 10,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Define the model
model <- keras_model_sequential()
# Add a single hidden layer with 64 neurons and ReLU activation
model %>%
layer_dense(units = 20, input_shape = c(1)) %>%
layer_activation_leaky_relu(alpha = 0.03)
# Add the output layer with a single neuron (regression problem)
model %>%
layer_dense(units = 1,use_bias = F)
# Compile the model
model %>% compile(
loss = 'mean_squared_error', # You can choose a different loss function if needed
optimizer = optimizer_adam() # You can choose a different optimizer if needed
)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 10,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Define the model
model <- keras_model_sequential()
# Add a single hidden layer with 64 neurons and ReLU activation
model %>%
layer_dense(units = 20, input_shape = c(1)) %>%
layer_activation_leaky_relu(alpha = 0.03)
# Add the output layer with a single neuron (regression problem)
model %>%
layer_dense(units = 1,use_bias = F)
# Compile the model
model %>% compile(
loss = 'mean_squared_error', # You can choose a different loss function if needed
optimizer = optimizer_adam() # You can choose a different optimizer if needed
)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 1,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
search_state <- initialize_search(param_len = weight_num,lowlim=lowlim,highlim=highlim,config = search.config(deg=1))
genstate = F
for (itt in 1:7){
tempconfig = search.config(gen = genstate,deg=1,search_samples = ls*3*2,search_mag=0.1,revert_best = T)
search_state <- step_search(desired_values = ydat,search_state,ineq_w = c(),eq_w=c(),config = tempconfig)
modout = model_function(as.vector(search_state$poly_recs[1,]))
surrogate_out <- surrogate_model(as.vector(search_state$poly_recs[1,]),search_state$polyouts,search_state$centersaves,deg=1)
if (mean(abs((modout-surrogate_out)/modout))>10){
genstate = T
}else{
genstate = F
}
par(oma=c(0, 0, 0, 5))
matplot(xdat,search_state$current_pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(search_state$poly_recs[1,])),col="red")
title("Surrogate Search Optimization")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(search_state$xpins)[1]))
search_state <- initialize_search(param_len = weight_num,lowlim=lowlim,highlim=highlim,config = search.config(deg=1))
genstate = F
for (itt in 1:7){
tempconfig = search.config(gen = genstate,deg=1,search_samples = weight_num*2,search_mag=0.1,revert_best = T)
search_state <- step_search(desired_values = ydat,search_state,ineq_w = c(),eq_w=c(),config = tempconfig)
modout = model_function(as.vector(search_state$poly_recs[1,]))
surrogate_out <- surrogate_model(as.vector(search_state$poly_recs[1,]),search_state$polyouts,search_state$centersaves,deg=1)
if (mean(abs((modout-surrogate_out)/modout))>10){
genstate = T
}else{
genstate = F
}
par(oma=c(0, 0, 0, 5))
matplot(xdat,search_state$current_pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(search_state$poly_recs[1,])),col="red")
title("Surrogate Search Optimization")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(search_state$xpins)[1]))
usevec = c(sample(1:dim(search_state$xpins)[1],weight_num),dim(search_state$xpins)[1])
#usevec = (dim(search_state$xpins)[1]-ls*2):dim(search_state$xpins)[1]
#Swap into particle swarm
swarm_state <- convert_search_to_swarm(search_state = search_state,usevec = usevec,desired_values = ydat,param_len = weight_num,lowlim=lowlim,highlim=highlim,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=length(usevec),num_cluster=1,deg=1))
for (itt in 1:20){
tempcontrol = swarm.control(poly_w=0.2,stoch_w = 0.01)
swarm_state <- step_swarm(desired_values = ydat,swarm_state,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=length(usevec),num_cluster=1,swarm.control = tempcontrol))
par(oma=c(0, 0, 0, 5))
matplot(xdat,swarm_state$pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
surrogate_out <- surrogate_model(as.vector(swarm_state$poly_rec),swarm_state$polyouts,swarm_state$centersaves,deg=1)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(swarm_state$poly_rec),ls),col="red")
title("Search-Initialized Scedastic Surrogate Swarm Optimization")
legend(par('usr')[2], par('usr')[4], bty='n',
xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(swarm_state$allxp)[1]))
# Chunk 1: setup
knitr::opts_chunk$set(
collapse = TRUE,
comment = "#>"
)
# Chunk 2: setup
rm(list=ls())
library(ScedasticSurrogateSwarmR)
library(keras)
library(dplyr)
# Chunk 3
ndat = 100
# xdat=sort(runif(ndat,0,1))
# ydat = c(sin(8*xdat)/2)+0.6
xdat=sort(runif(ndat,-10,10))
ydat = c(sin(0.5*xdat)/2)+0.6
plot(xdat,ydat)
# Chunk 4
# Define the model
model <- keras_model_sequential()
# Add a single hidden layer with 64 neurons and ReLU activation
model %>%
layer_dense(units = 20, input_shape = c(1)) %>%
layer_activation_leaky_relu(alpha = 0.03)
# Add the output layer with a single neuron (regression problem)
model %>%
layer_dense(units = 1,use_bias = F)
# Compile the model
model %>% compile(
loss = 'mean_squared_error', # You can choose a different loss function if needed
optimizer = optimizer_adam() # You can choose a different optimizer if needed
)
# Chunk 5
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 1,
verbose = 2
)
# Chunk 6
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Chunk 7
# creation function to assign to weights to keras
weight_mat_orig <- get_weights(model)
weight_num <- length(unlist(weight_mat_orig))
lowlim=rep(-1,weight_num)
highlim=rep(1,weight_num)
# Chunk 8
assign_keras_weights <- function(new_array,weight_mat_orig){
#initialize weight matrix
weight_mat_new <- weight_mat_orig
arr_start <- 0
#Loop through weight matrices
for (iw in 1:length(weight_mat_orig)){
matdim = dim(weight_mat_orig[[iw]])
arr_end = arr_start+prod(matdim)
weight_mat_new[[iw]] = array(new_array[(arr_start+1):arr_end],dim=matdim)
arr_start = arr_end
}
return(weight_mat_new)
}
# Chunk 9
model_function <-function(params,ls){
new_weights <- assign_keras_weights(params,weight_mat_orig)
newmodel <- model
set_weights(newmodel,new_weights)
modval <- newmodel %>% predict(xdat,verbose = F)
return(modval)
}
#Equality Constraints
eq <- function(params){
constraints = c(
)
return(constraints)
}
#Inequality Constraints
# of form <= 0
ineq <- function(params){
constraints = c(
)
return(constraints)
}
# Chunk 10
swarm_state <- initialize_swarm(desired_values = ydat,param_len = weight_num,lowlim=lowlim,highlim=highlim,ineq_w = c(),eq_w = c(),config = swarm.config(swarm_size = 100))
for (itt in 1:50){
tempcontrol = swarm.control(poly_w=0.2,stoch_w = 0.01)
swarm_state <- step_swarm(desired_values = ydat,swarm_state,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=dim(swarm_state$x.p)[1],num_cluster=1,swarm.control = tempcontrol))
matplot(xdat,swarm_state$pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
surrogate_out <- surrogate_model(as.vector(swarm_state$poly_rec),swarm_state$polyouts,swarm_state$centersaves,deg=1)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(swarm_state$poly_rec)),col="red")
title("Scedastic Surrogate Swarm Optimization")
legend(par('usr')[2], par('usr')[4], bty='n',
xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(swarm_state$allxp)[1]))
# Chunk 11
search_state <- initialize_search(param_len = weight_num,lowlim=lowlim,highlim=highlim,config = search.config(deg=1))
genstate = F
for (itt in 1:7){
tempconfig = search.config(gen = genstate,deg=1,search_samples = weight_num*2,search_mag=0.1,revert_best = T)
search_state <- step_search(desired_values = ydat,search_state,ineq_w = c(),eq_w=c(),config = tempconfig)
modout = model_function(as.vector(search_state$poly_recs[1,]))
surrogate_out <- surrogate_model(as.vector(search_state$poly_recs[1,]),search_state$polyouts,search_state$centersaves,deg=1)
if (mean(abs((modout-surrogate_out)/modout))>10){
genstate = T
}else{
genstate = F
}
par(oma=c(0, 0, 0, 5))
matplot(xdat,search_state$current_pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(search_state$poly_recs[1,])),col="red")
title("Surrogate Search Optimization")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(search_state$xpins)[1]))
# Chunk 12
usevec = c(sample(1:dim(search_state$xpins)[1],weight_num),dim(search_state$xpins)[1])
#usevec = (dim(search_state$xpins)[1]-ls*2):dim(search_state$xpins)[1]
#Swap into particle swarm
swarm_state <- convert_search_to_swarm(search_state = search_state,usevec = usevec,desired_values = ydat,param_len = weight_num,lowlim=lowlim,highlim=highlim,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=length(usevec),num_cluster=1,deg=1))
for (itt in 1:20){
tempcontrol = swarm.control(poly_w=0.2,stoch_w = 0.01)
swarm_state <- step_swarm(desired_values = ydat,swarm_state,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=length(usevec),num_cluster=1,swarm.control = tempcontrol))
par(oma=c(0, 0, 0, 5))
matplot(xdat,swarm_state$pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
surrogate_out <- surrogate_model(as.vector(swarm_state$poly_rec),swarm_state$polyouts,swarm_state$centersaves,deg=1)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(swarm_state$poly_rec),ls),col="red")
title("Search-Initialized Scedastic Surrogate Swarm Optimization")
legend(par('usr')[2], par('usr')[4], bty='n',
xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(swarm_state$allxp)[1]))
# Define the model
model <- keras_model_sequential()
# Add a single hidden layer with 64 neurons and ReLU activation
model %>%
# layer_dense(units = 20, input_shape = c(1)) %>%
# layer_activation_leaky_relu(alpha = 0.03)
layer_dense(units = 20, input_shape = c(1), activation = "relu")
# Add the output layer with a single neuron (regression problem)
model %>%
layer_dense(units = 1,use_bias = F)
# Compile the model
model %>% compile(
loss = 'mean_squared_error', # You can choose a different loss function if needed
optimizer = optimizer_adam() # You can choose a different optimizer if needed
)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 1,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Define the model
model <- keras_model_sequential()
# Add a single hidden layer with 64 neurons and ReLU activation
model %>%
layer_dense(units = 20, input_shape = c(1)) %>%
layer_activation_leaky_relu(alpha = 0.03)
#layer_dense(units = 20, input_shape = c(1), activation = "relu")
# Add the output layer with a single neuron (regression problem)
model %>%
layer_dense(units = 1,use_bias = F)
# Compile the model
model %>% compile(
loss = 'mean_squared_error', # You can choose a different loss function if needed
optimizer = optimizer_adam() # You can choose a different optimizer if needed
)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
#batch_size = 1,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 1,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# Define the model
model <- keras_model_sequential()
# Add a single hidden layer with 64 neurons and ReLU activation
model %>%
layer_dense(units = 100, input_shape = c(1)) %>%
layer_activation_leaky_relu(alpha = 0.03)
#layer_dense(units = 20, input_shape = c(1), activation = "relu")
# Add the output layer with a single neuron (regression problem)
model %>%
layer_dense(units = 1,use_bias = F)
# Compile the model
model %>% compile(
loss = 'mean_squared_error', # You can choose a different loss function if needed
optimizer = optimizer_adam() # You can choose a different optimizer if needed
)
# Fit the model to your data
history <- model %>%
fit(
x = xdat,
y = ydat,
epochs = 100, # You can adjust the number of epochs as needed
batch_size = 1,
verbose = 2
)
y_model <- model %>% predict(xdat)
plot(xdat,ydat)
lines(xdat,y_model)
# creation function to assign to weights to keras
weight_mat_orig <- get_weights(model)
weight_num <- length(unlist(weight_mat_orig))
lowlim=rep(-1,weight_num)
highlim=rep(1,weight_num)
assign_keras_weights <- function(new_array,weight_mat_orig){
#initialize weight matrix
weight_mat_new <- weight_mat_orig
arr_start <- 0
#Loop through weight matrices
for (iw in 1:length(weight_mat_orig)){
matdim = dim(weight_mat_orig[[iw]])
arr_end = arr_start+prod(matdim)
weight_mat_new[[iw]] = array(new_array[(arr_start+1):arr_end],dim=matdim)
arr_start = arr_end
}
return(weight_mat_new)
}
model_function <-function(params,ls){
new_weights <- assign_keras_weights(params,weight_mat_orig)
newmodel <- model
set_weights(newmodel,new_weights)
modval <- newmodel %>% predict(xdat,verbose = F)
return(modval)
}
#Equality Constraints
eq <- function(params){
constraints = c(
)
return(constraints)
}
#Inequality Constraints
# of form <= 0
ineq <- function(params){
constraints = c(
)
return(constraints)
}
swarm_state <- initialize_swarm(desired_values = ydat,param_len = weight_num,lowlim=lowlim,highlim=highlim,ineq_w = c(),eq_w = c(),config = swarm.config(swarm_size = 100))
for (itt in 1:50){
tempcontrol = swarm.control(poly_w=0.2,stoch_w = 0.01)
swarm_state <- step_swarm(desired_values = ydat,swarm_state,ineq_w = c(),eq_w=c(),config = swarm.config(swarm_size=dim(swarm_state$x.p)[1],num_cluster=1,swarm.control = tempcontrol))
par(oma=c(0, 0, 0, 5))
matplot(xdat,swarm_state$pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
surrogate_out <- surrogate_model(as.vector(swarm_state$poly_rec),swarm_state$polyouts,swarm_state$centersaves,deg=1)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(swarm_state$poly_rec)),col="red")
title("Scedastic Surrogate Swarm Optimization")
legend(par('usr')[2], par('usr')[4], bty='n',
xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(swarm_state$allxp)[1]))
search_state <- initialize_search(param_len = weight_num,lowlim=lowlim,highlim=highlim,config = search.config(deg=1))
genstate = F
for (itt in 1:7){
tempconfig = search.config(gen = genstate,deg=1,search_samples = weight_num*2,search_mag=0.1,revert_best = T)
search_state <- step_search(desired_values = ydat,search_state,ineq_w = c(),eq_w=c(),config = tempconfig)
modout = model_function(as.vector(search_state$poly_recs[1,]))
surrogate_out <- surrogate_model(as.vector(search_state$poly_recs[1,]),search_state$polyouts,search_state$centersaves,deg=1)
if (mean(abs((modout-surrogate_out)/modout))>10){
genstate = T
}else{
genstate = F
}
par(oma=c(0, 0, 0, 5))
matplot(xdat,search_state$current_pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(search_state$poly_recs[1,])),col="red")
title("Surrogate Search Optimization")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(search_state$xpins)[1]))
search_state <- initialize_search(param_len = weight_num,lowlim=lowlim,highlim=highlim,config = search.config(deg=1))
genstate = F
for (itt in 1:7){
tempconfig = search.config(gen = genstate,deg=1,search_samples = weight_num*2,search_mag=0.1,revert_best = T)
search_state <- step_search(desired_values = ydat,search_state,ineq_w = c(),eq_w=c(),config = tempconfig)
modout = model_function(as.vector(search_state$poly_recs[1,]))
surrogate_out <- surrogate_model(as.vector(search_state$poly_recs[1,]),search_state$polyouts,search_state$centersaves,deg=1)
if (mean(abs((modout-surrogate_out)/modout))>10){
genstate = T
}else{
genstate = F
}
par(oma=c(0, 0, 0, 5))
matplot(xdat,search_state$current_pout,type="l",col="grey",xlab = NULL,ylab="value",ylim = c(0,1.2))
points(xdat,ydat)
lines(xdat,surrogate_out,col="blue")
lines(xdat,model_function(as.vector(search_state$poly_recs[1,])),col="red")
title("Surrogate Search Optimization")
legend(par('usr')[2], par('usr')[4], bty='n', xpd=NA,c("Observed","Modeled","Predicted","Actual"),fill=c("black","grey","blue","red"))
}
print(paste("Number of function evals: ",dim(search_state$xpins)[1]))
